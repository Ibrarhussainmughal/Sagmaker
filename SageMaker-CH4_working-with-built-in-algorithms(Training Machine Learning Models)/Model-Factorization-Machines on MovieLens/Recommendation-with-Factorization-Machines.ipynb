{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-05-18 14:36:42--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4.7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  793K 6s\n",
      "    50K .......... .......... .......... .......... ..........  2% 1.41M 5s\n",
      "   100K .......... .......... .......... .......... ..........  3%  165M 3s\n",
      "   150K .......... .......... .......... .......... ..........  4% 86.8M 2s\n",
      "   200K .......... .......... .......... .......... ..........  5% 1.44M 2s\n",
      "   250K .......... .......... .......... .......... ..........  6%  150M 2s\n",
      "   300K .......... .......... .......... .......... ..........  7%  132M 2s\n",
      "   350K .......... .......... .......... .......... ..........  8% 19.8M 1s\n",
      "   400K .......... .......... .......... .......... ..........  9% 1.56M 2s\n",
      "   450K .......... .......... .......... .......... .......... 10%  148M 1s\n",
      "   500K .......... .......... .......... .......... .......... 11% 75.8M 1s\n",
      "   550K .......... .......... .......... .......... .......... 12%  176M 1s\n",
      "   600K .......... .......... .......... .......... .......... 13%  141M 1s\n",
      "   650K .......... .......... .......... .......... .......... 14% 95.2M 1s\n",
      "   700K .......... .......... .......... .......... .......... 15% 47.1M 1s\n",
      "   750K .......... .......... .......... .......... .......... 16%  126M 1s\n",
      "   800K .......... .......... .......... .......... .......... 17%  196M 1s\n",
      "   850K .......... .......... .......... .......... .......... 18% 1.57M 1s\n",
      "   900K .......... .......... .......... .......... .......... 19%  179M 1s\n",
      "   950K .......... .......... .......... .......... .......... 20%  152M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 21%  186M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 22% 55.4M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 23%  146M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 24%  131M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 25%  148M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 27%  133M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 28%  135M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 29%  149M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 30%  120M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 31% 82.0M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 32%  185M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 33% 1.70M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 34% 41.4M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 35%  253M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 36% 83.6M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 37%  159M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 38%  148M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 39%  142M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 40%  110M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 41%  111M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 42%  146M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 43%  146M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 44%  144M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 45%  163M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 46%  135M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 47%  110M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 48%  139M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 49%  152M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 50%  145M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 51%  204M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 53%  103M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 54%  215M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 55%  125M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 56%  126M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 57%  205M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 58% 95.0M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 59%  116M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 60%  221M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 61%  158M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 62%  117M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 63%  163M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 64%  164M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 65% 2.01M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 66% 75.0M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 67% 45.0M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 68%  318M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 69%  324M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 70% 72.1M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 71%  224M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 72%  176M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 73% 65.4M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 74%  149M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 75%  142M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 76% 70.3M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 77%  171M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 79%  136M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 80%  143M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 81%  109M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 82%  306M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 83%  118M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 84%  139M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 85%  114M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 86%  210M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 87%  145M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 88%  112M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 89%  115M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 90%  223M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 91%  102M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 92%  151M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 93%  171M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 94%  103M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 95%  279M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 96% 2.20M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 97% 50.8M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 98% 56.1M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 99% 57.7M 0s\n",
      "  4800K ........                                              100%  212M=0.3s\n",
      "\n",
      "2022-05-18 14:36:43 (15.3 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "unzip ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ml-100k/ml-100k\n",
      "460\t13\t3\t882912371\n",
      "730\t257\t5\t880310541\n",
      "360\t879\t3\t880354094\n",
      "320\t1215\t1\t884749097\n",
      "560\t235\t2\t879976867\n"
     ]
    }
   ],
   "source": [
    "%cd ml-100k\n",
    "!shuf ua.base -o ua.base.shuffled\n",
    "!head -5 ua.base.shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 943\n",
    "num_movies = 1682\n",
    "num_features = num_users+num_movies\n",
    "num_ratings_train = 90570\n",
    "num_ratings_test = 9430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def loadDataset(filename, lines, columns):\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter='\\t')\n",
    "        for userId,movieId,rating,timestamp in samples:\n",
    "            X[line,int(userId)-1] = 1\n",
    "            X[line,int(num_users)+int(movieId)-1] = 1\n",
    "            Y.append(int(rating))\n",
    "            line=line+1       \n",
    "    Y=np.array(Y).astype('float32')\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = loadDataset('ua.base.shuffled', num_ratings_train, num_features)\n",
    "X_test, Y_test = loadDataset('ua.test', num_ratings_test, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 2625)\n",
      "(90570,)\n",
      "(9430, 2625)\n",
      "(9430,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (num_ratings_train, num_features)\n",
    "assert Y_train.shape == (num_ratings_train, )\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (num_ratings_test, num_features)\n",
    "assert Y_test.shape  == (num_ratings_test, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BytesIO object at 0x7f169c87ddd0>\n",
      "<_io.BytesIO object at 0x7f169c87ddd0>\n",
      "s3://sagemaker-us-east-1-607098578469/fm-movielens/train/train.protobuf\n",
      "s3://sagemaker-us-east-1-607098578469/fm-movielens/test/test.protobuf\n",
      "Output: s3://sagemaker-us-east-1-607098578469/fm-movielens/output\n"
     ]
    }
   ],
   "source": [
    "import io, boto3\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    # use smac.write_numpy_to_dense_tensor(buf, feature, label) for numpy arrays\n",
    "    buf.seek(0)\n",
    "    print(buf)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "train_data = writeDatasetToProtobuf(X_train, Y_train, bucket, train_prefix, train_key)    \n",
    "test_data  = writeDatasetToProtobuf(X_test, Y_test, bucket, test_prefix, test_key)    \n",
    "  \n",
    "print(train_data)\n",
    "print(test_data)\n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "container = image_uris.retrieve('factorization-machines', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:40:22 Starting - Starting the training job...\n",
      "2022-05-18 14:40:40 Starting - Preparing the instances for trainingProfilerReport-1652884821: InProgress\n",
      ".........\n",
      "2022-05-18 14:42:15 Downloading - Downloading input data...\n",
      "2022-05-18 14:42:45 Training - Downloading the training image...\n",
      "2022-05-18 14:43:21 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '10', 'feature_dim': '2625', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] Final configuration: {'epochs': '10', 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '2625', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 WARNING 139621224789824] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] Using default worker.\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:26.355] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:26.366] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 17, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] nvidia-smi: took 0.036 seconds to run.\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885006.348509, \"EndTime\": 1652885006.407583, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 43.1218147277832, \"count\": 1, \"min\": 43.1218147277832, \"max\": 43.1218147277832}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885006.4077024, \"EndTime\": 1652885006.4077418, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[14:43:26] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206339.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[14:43:26] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206339.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=3.688786586992395\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=13.607146484375\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:26 INFO 139621224789824] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=3.502231689453125\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:27.179] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 747, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.744700060852097\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #quality_metric: host=algo-1, epoch=0, train mse <loss>=3.0439783023373113\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=1.3832335687993647\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885006.4076524, \"EndTime\": 1652885007.1803703, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"update.time\": {\"sum\": 772.3596096038818, \"count\": 1, \"min\": 772.3596096038818, \"max\": 772.3596096038818}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885006.4079833, \"EndTime\": 1652885007.1805923, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 91570.0, \"count\": 1, \"min\": 91570, \"max\": 91570}, \"Total Batches Seen\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=117210.98708198176 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.148805957049808\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.319755126953125\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.9658468627929687\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:27.975] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 791, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.1316100470865187\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.2805412986671532\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.9465591283106541\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885007.1804502, \"EndTime\": 1652885007.9757767, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 794.5749759674072, \"count\": 1, \"min\": 794.5749759674072, \"max\": 794.5749759674072}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885007.1811733, \"EndTime\": 1652885007.9760158, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 182140.0, \"count\": 1, \"min\": 182140, \"max\": 182140}, \"Total Batches Seen\": {\"sum\": 183.0, \"count\": 1, \"min\": 183, \"max\": 183}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:27 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=113928.99324934073 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=1.1319836407345403\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=1.281386962890625\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.9521445922851562\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:28.869] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 891, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.113759631289602\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.2404605162903504\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.9297186634776357\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885007.9758575, \"EndTime\": 1652885008.8711326, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 894.5093154907227, \"count\": 1, \"min\": 894.5093154907227, \"max\": 894.5093154907227}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885007.9765959, \"EndTime\": 1652885008.8717494, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 272710.0, \"count\": 1, \"min\": 272710, \"max\": 272710}, \"Total Batches Seen\": {\"sum\": 274.0, \"count\": 1, \"min\": 274, \"max\": 274}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=101139.35785187755 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=1.1129917935105698\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=1.238750732421875\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:28 INFO 139621224789824] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.9347614135742187\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:29.547] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 673, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:29 INFO 139621224789824] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=1.0946997146142066\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:29 INFO 139621224789824] #quality_metric: host=algo-1, epoch=3, train mse <loss>=1.1983674651764251\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:29 INFO 139621224789824] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.9107539907602163\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885008.8714359, \"EndTime\": 1652885009.5481987, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 675.3606796264648, \"count\": 1, \"min\": 675.3606796264648, \"max\": 675.3606796264648}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:29 INFO 139621224789824] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885008.872814, \"EndTime\": 1652885009.5484247, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 363280.0, \"count\": 1, \"min\": 363280, \"max\": 363280}, \"Total Batches Seen\": {\"sum\": 365.0, \"count\": 1, \"min\": 365, \"max\": 365}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:29 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=134029.42088121572 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:29 INFO 139621224789824] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=1.0932958320451\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:29 INFO 139621224789824] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=1.1952957763671874\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:29 INFO 139621224789824] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=0.9156434326171875\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:30.267] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 717, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=1.075532947667175\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=4, train mse <loss>=1.1567711215176426\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.8905355311802455\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885009.5482655, \"EndTime\": 1652885010.2683613, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 719.245195388794, \"count\": 1, \"min\": 719.245195388794, \"max\": 719.245195388794}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885009.5490868, \"EndTime\": 1652885010.2686203, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 453850.0, \"count\": 1, \"min\": 453850, \"max\": 453850}, \"Total Batches Seen\": {\"sum\": 456.0, \"count\": 1, \"min\": 456, \"max\": 456}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=125850.25483908692 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=1.0742713623479578\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=1.1540589599609374\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=0.8962852783203125\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:30.889] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 619, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=1.0573633580454123\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=5, train mse <loss>=1.1180172709370708\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=0.870197510436341\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885010.2684562, \"EndTime\": 1652885010.8907948, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 621.6084957122803, \"count\": 1, \"min\": 621.6084957122803, \"max\": 621.6084957122803}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885010.2691598, \"EndTime\": 1652885010.891013, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 544420.0, \"count\": 1, \"min\": 544420, \"max\": 544420}, \"Total Batches Seen\": {\"sum\": 547.0, \"count\": 1, \"min\": 547, \"max\": 547}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=145616.128281439 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=1.056750932897477\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=1.1167225341796876\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:30 INFO 139621224789824] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=0.8774549560546875\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:31.501] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 606, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:31 INFO 139621224789824] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=1.04080338437146\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:31 INFO 139621224789824] #quality_metric: host=algo-1, epoch=6, train mse <loss>=1.0832716849190849\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:31 INFO 139621224789824] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=0.8507962747091775\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885010.8908632, \"EndTime\": 1652885011.50236, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 610.7051372528076, \"count\": 1, \"min\": 610.7051372528076, \"max\": 610.7051372528076}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:31 INFO 139621224789824] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885010.8916278, \"EndTime\": 1652885011.5026145, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 634990.0, \"count\": 1, \"min\": 634990, \"max\": 634990}, \"Total Batches Seen\": {\"sum\": 638.0, \"count\": 1, \"min\": 638, \"max\": 638}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:31 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=148203.07108188234 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:31 INFO 139621224789824] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=1.041116801485759\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:31 INFO 139621224789824] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=1.0839241943359375\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:31 INFO 139621224789824] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=0.8598677978515625\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:32.134] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 629, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=1.0261174852158093\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=7, train mse <loss>=1.0529170934656165\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=0.8334264419052627\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885011.5024378, \"EndTime\": 1652885012.1358225, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 632.4317455291748, \"count\": 1, \"min\": 632.4317455291748, \"max\": 632.4317455291748}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885011.5033581, \"EndTime\": 1652885012.1360567, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 725560.0, \"count\": 1, \"min\": 725560, \"max\": 725560}, \"Total Batches Seen\": {\"sum\": 729.0, \"count\": 1, \"min\": 729, \"max\": 729}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=143119.1345295653 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=1.0274644653412897\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=1.0556832275390624\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=0.8442221069335938\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:32.757] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 619, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=1.0133435726071331\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=8, train mse <loss>=1.026865196144188\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=0.8186638740288033\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885012.1358902, \"EndTime\": 1652885012.7580829, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 621.4652061462402, \"count\": 1, \"min\": 621.4652061462402, \"max\": 621.4652061462402}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885012.1365879, \"EndTime\": 1652885012.7583916, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 816130.0, \"count\": 1, \"min\": 816130, \"max\": 816130}, \"Total Batches Seen\": {\"sum\": 820.0, \"count\": 1, \"min\": 820, \"max\": 820}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=145631.7031840321 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=1.0157221107419097\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=1.03169140625\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:32 INFO 139621224789824] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=0.8303230590820313\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:33.396] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 635, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=1.0023784159348061\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #quality_metric: host=algo-1, epoch=9, train mse <loss>=1.004762488731971\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=0.8065103444529104\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #quality_metric: host=algo-1, train rmse <loss>=1.0023784159348061\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #quality_metric: host=algo-1, train mse <loss>=1.004762488731971\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #quality_metric: host=algo-1, train absolute_loss <loss>=0.8065103444529104\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885012.7581923, \"EndTime\": 1652885013.3967967, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 637.7768516540527, \"count\": 1, \"min\": 637.7768516540527, \"max\": 637.7768516540527}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885012.7589977, \"EndTime\": 1652885013.3970144, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 906700.0, \"count\": 1, \"min\": 906700, \"max\": 906700}, \"Total Batches Seen\": {\"sum\": 911.0, \"count\": 1, \"min\": 911, \"max\": 911}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #throughput_metric: host=algo-1, train throughput=141926.09677010795 records/second\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 WARNING 139621224789824] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885013.3968651, \"EndTime\": 1652885013.3993828, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 2.0618438720703125, \"count\": 1, \"min\": 2.0618438720703125, \"max\": 2.0618438720703125}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] Saved checkpoint to \"/tmp/tmpjte33m2p/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:33.409] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 7053, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[34m[2022-05-18 14:43:33.448] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 39, \"num_examples\": 10, \"num_bytes\": 603520}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885013.4088674, \"EndTime\": 1652885013.4490185, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Total Batches Seen\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Max Records Seen Between Resets\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #test_score (algo-1) : ('rmse', 1.0186678377731897)\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #test_score (algo-1) : ('mse', 1.0376841637135057)\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #test_score (algo-1) : ('absolute_loss', 0.8372256848364594)\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #quality_metric: host=algo-1, test rmse <loss>=1.0186678377731897\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #quality_metric: host=algo-1, test mse <loss>=1.0376841637135057\u001b[0m\n",
      "\u001b[34m[05/18/2022 14:43:33 INFO 139621224789824] #quality_metric: host=algo-1, test absolute_loss <loss>=0.8372256848364594\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652885013.399436, \"EndTime\": 1652885013.4510355, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 24.239778518676758, \"count\": 1, \"min\": 24.239778518676758, \"max\": 24.239778518676758}, \"totaltime\": {\"sum\": 7132.762670516968, \"count\": 1, \"min\": 7132.762670516968, \"max\": 7132.762670516968}}}\u001b[0m\n",
      "\n",
      "2022-05-18 14:43:52 Uploading - Uploading generated training model\n",
      "2022-05-18 14:43:52 Completed - Training job completed\n",
      "Training seconds: 97\n",
      "Billable seconds: 97\n"
     ]
    }
   ],
   "source": [
    "fm = sagemaker.estimator.Estimator(container,\n",
    "                                   role=sagemaker.get_execution_role(),\n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.c5.xlarge',\n",
    "                                   output_path=output_prefix\n",
    "                                   )\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=num_features,\n",
    "                      predictor_type='regressor',\n",
    "                      num_factors=64,\n",
    "                      epochs=10)\n",
    "\n",
    "fm.fit({'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'fm-movielens-100k'\n",
    "fm_predictor = fm.deploy(endpoint_name=endpoint_name,\n",
    "                         instance_type='ml.t2.medium', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "       js = {'instances': []}\n",
    "       for row in data:\n",
    "              js['instances'].append({'features': row.tolist()})\n",
    "       return json.dumps(js)\n",
    "\n",
    "fm_predictor.serializer = FMSerializer()\n",
    "fm_predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'score': 3.38551664352417}, {'score': 3.44069504737854}, {'score': 3.6563968658447266}]}\n"
     ]
    }
   ],
   "source": [
    "result = fm_predictor.predict(X_test[:3].toarray())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
